{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7b3adf-ebaa-49df-a7ee-1d8129157c51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8d9669-ca0e-497e-be62-cc8954a62a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class StochasticBernoulliMixture:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, batch_size=100, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            self.get_Neff()\n",
    "            self.get_mu(self.x)\n",
    "            self.get_pi()\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa142183-e84b-42e6-bbed-b86cf6e2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Suppress the FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from ucimlrepo import fetch_ucirepo   \n",
    "# fetch dataset \n",
    "zoo = fetch_ucirepo(id=111) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = zoo.data.features\n",
    "y = zoo.data.targets \n",
    "zoo_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "zoo_df = zoo_df.dropna()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_df = zoo_df.copy()\n",
    "for column in zoo_df.columns:\n",
    "        if zoo_df[column].dtype == 'object':\n",
    "            encoded_df[column] = encoder.fit_transform(zoo_df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d14cb0-c53f-46b5-9c71-438696451613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Bernoulli Mixture:\n",
      "Adjusted Rand Index (ARI): 0.5973920064427068\n",
      "Normalized Mutual Information (NMI): 0.6342850921689538\n",
      "Folkes-Mallows Index (FMI): 0.7226653463342378\n"
     ]
    }
   ],
   "source": [
    "# Perform clustering\n",
    "n_components = 3  # Number of clusters\n",
    "max_iter = 100  # Maximum number of iterations\n",
    "batch_size = 50  # Size of mini-batch\n",
    "\n",
    "X =  encoded_df.iloc[:, :-1]\n",
    "true_labels = encoded_df.iloc[:, -1]\n",
    "# Initialize the StochasticBernoulliMixture model\n",
    "stochastic_bmm = StochasticBernoulliMixture(n_components=n_components, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "# Fit the model to the binary encoded data\n",
    "stochastic_bmm.fit(X)\n",
    "\n",
    "# Extract cluster assignments\n",
    "labels = stochastic_bmm.predict(X)\n",
    "if true_labels is not None:\n",
    "        fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Stochastic Bernoulli Mixture:\")\n",
    "print(\"Adjusted Rand Index (ARI):\", ari)\n",
    "print(\"Normalized Mutual Information (NMI):\", nmi)\n",
    "print(\"Folkes-Mallows Index (FMI):\", fmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3211017-c62f-4f1c-bc40-b951cff19eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KModes:\n",
      "Adjusted Rand Index (ARI): 0.5973920064427068\n",
      "Normalized Mutual Information (NMI): 0.6342850921689538\n",
      "Folkes-Mallows Index (FMI): 0.7226653463342378\n"
     ]
    }
   ],
   "source": [
    "km = KModes(n_clusters = 3, max_iter = 100)\n",
    "km_labels = km.fit_predict(X)\n",
    "if true_labels is not None:\n",
    "        fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "        ari = adjusted_rand_score(true_labels, labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"KModes:\")\n",
    "print(\"Adjusted Rand Index (ARI):\", ari)\n",
    "print(\"Normalized Mutual Information (NMI):\", nmi)\n",
    "print(\"Folkes-Mallows Index (FMI):\", fmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d96db3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "$\\textbf{Report}$\n",
    "<br>\n",
    "<br>\n",
    "The similarity in performance measures between the Stochastic Bernoulli Mixture and KModes indicates that both clustering algorithms performed similarly on the provided dataset. The ratings for the Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Folkes-Mallows Index (FMI) are consistent across both approaches, showing that they produced clusterings that were similar to ground truth labels and of equivalent quality. This shows that the underlying structures recorded by the two techniques are comparable, resulting in identical clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59494eea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
